{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Feature Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1bKT4VwkN43yxHH6lT1sE_QN6-vnMti2E",
      "authorship_tag": "ABX9TyPGDiQTNQej4YXYxgjELaFq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheensta/retail_products_ensemble_deep_learning/blob/main/Google%20Colab%20Notebooks/NLP_Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iitpTGSrwM6I"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6MoCF4-wIqv"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/df_NLP.csv')\n",
        "df['description_clean'] = df['description_clean'].astype(str)\n",
        "corpus = df['description_clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N59dXOZ-3xxz"
      },
      "source": [
        "Word Similarity with word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in7HLLO0wCw5"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d3wo3glwKj_"
      },
      "source": [
        "feature_size = 100    # Word vector dimensionality  \n",
        "window_context = 30          # Context window size                                                                                    \n",
        "min_word_count = 1   # Minimum word count                        \n",
        "sample = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "tokenized_corpus = [wpt.tokenize(document) for document in corpus]\n",
        "\n",
        "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
        "                          window=window_context, min_count=min_word_count,\n",
        "                          sample=sample, iter=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDZTu3DATjq4"
      },
      "source": [
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "    \n",
        "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "    \n",
        "    for word in words:\n",
        "        if word in vocabulary: \n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model[word])\n",
        "    \n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "        \n",
        "    return feature_vector\n",
        "\n",
        "def averaged_word_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)\n",
        "\n",
        "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model,\n",
        "                                             num_features=feature_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LTditg_Iuy4"
      },
      "source": [
        "pd.DataFrame(w2v_feature_array).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T24tablv3594"
      },
      "source": [
        "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
        "                  for search_term in ['pet','electronics','beauty','industrial','baby','arts','outdoors']}\n",
        "similar_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4exuDLpNTEZ9",
        "collapsed": true
      },
      "source": [
        "#@title\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import keras\n",
        "#le = LabelEncoder()\n",
        "#le.fit(df['categories'])\n",
        "#y = list(le.transform(df['categories']))\n",
        "\n",
        "#num_classes = 21\n",
        "y = list(df['categories'])\n",
        "X = w2v_feature_array\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "#y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVXUHBFwKOXt"
      },
      "source": [
        "w2v_feature_array = pd.DataFrame(X)\n",
        "a = pd.concat([w2v_feature_array, y], axis = 1)\n",
        "pd.DataFrame(a).to_csv('/content/drive/MyDrive/Colab Notebooks/w2v_feature_array_evaluation.csv', index= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztj6Byj3TEek",
        "collapsed": true
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "param_grid = [{}]\n",
        "lg = GridSearchCV(LogisticRegression(), \n",
        "                           param_grid,\n",
        "                           cv=KFold(n_splits=10, \n",
        "                                              random_state=42).split(X_train, y_train), \n",
        "                           verbose=1)\n",
        "y_preds_lg = lg.fit(X_train, y_train).predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWBVQseaTEhB",
        "collapsed": true
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report_lg = classification_report( y_test, y_preds_lg)\n",
        "print(report_lg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67ZV_E3DXp01"
      },
      "source": [
        "lg.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_9zd_-z-Ocz"
      },
      "source": [
        "param_grid = [{}]\n",
        "rf = GridSearchCV(RandomForestClassifier(), \n",
        "                           param_grid,\n",
        "                           cv=KFold(n_splits=10, \n",
        "                                              random_state=42).split(X_train, y_train), \n",
        "                           verbose=1)\n",
        "y_preds_rf = rf.fit(X_train, y_train).predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDP4Ou32obSH"
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG9ouKfsTElo"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report_rf = classification_report( y_test, y_preds_rf)\n",
        "print(report_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAQZGIZ8lELf"
      },
      "source": [
        "pd.DataFrame(y_preds_rf).to_csv('/content/drive/MyDrive/Colab Notebooks/y_preds_rf.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_NeNd2fplNR"
      },
      "source": [
        "import joblib\n",
        "joblib.dump(rf.best_estimator_, 'NLP_rf.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYBIkhC6dsgL"
      },
      "source": [
        "rf.best_estimator_.predict(X_test).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfuyhuCZex80"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import keras\n",
        "le = LabelEncoder()\n",
        "le.fit(df['categories'])\n",
        "y = list(le.transform(df['categories']))\n",
        "\n",
        "\n",
        "num_classes = 21\n",
        "\n",
        "X = w2v_feature_array\n",
        "y = list(le.transform(df['categories']))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "X_train = np.array(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkTBFU5DM10f"
      },
      "source": [
        "#xgboost hyperparameter tuning\n",
        "import xgboost as xgb\n",
        "from scipy import stats\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
        "from sklearn.metrics import f1_score\n",
        "X = pd.DataFrame(w2v_feature_array)\n",
        "xgb = XGBClassifier(objective = 'multiclass:softmax')\n",
        "\n",
        "param_grid = [{}]\n",
        "clf_xgb = GridSearchCV(xgb, \n",
        "                           param_grid,\n",
        "                           cv=KFold(n_splits=10, \n",
        "                                              random_state=42).split(X_train, y_train), \n",
        "                           verbose=1)\n",
        "y_preds_xbg = clf_xgb.fit(X_train, y_train).predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBMnESb3lduN"
      },
      "source": [
        "pd.DataFrame(y_preds_xbg).to_csv('/content/drive/MyDrive/Colab Notebooks/y_preds_xgb.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShIT1Lb7A3Lw"
      },
      "source": [
        "joblib.dump(clf_xgb.best_estimator_, '/content/drive/MyDrive/models/NLP_XGB.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6ZDRTckrVT4"
      },
      "source": [
        "report_xgb = classification_report( y_test, y_preds_xbg)\n",
        "print(report_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uXQpi00_fKv"
      },
      "source": [
        "import joblib\n",
        "filename = 'NLP_xgb.sav'\n",
        "joblib.dump(clf_xgb, filename)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIYErUNwYbYF"
      },
      "source": [
        "Keras Word Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO5CN2ESTEnX"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = list(df['description_clean'])\n",
        "le = LabelEncoder()\n",
        "le.fit(df['categories'])\n",
        "y = list(le.transform(df['categories']))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "import keras\n",
        "num_classes = 21\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuGrhmUaTEpQ"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 250\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZMkNWu9jQ3R"
      },
      "source": [
        "#TRAINING A SIMPLE DEEP LEARNING MODEL (accuracy similar to Random Forest)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(21, activation='softmax'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe1kKYVIjQ5b"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxbsqx68jQ7f"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUy9Xrq76KWJ"
      },
      "source": [
        "model.save('/content/drive/MyDrive/models/NLP_custom_trainedsimple DL.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0Uk6DsKgdQS"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/drive/MyDrive/Colab Notebooks/glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9WDmovUhUq2"
      },
      "source": [
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGygebe_jPLS"
      },
      "source": [
        "#Embedding with pre-trained glove model\n",
        "\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=True)\n",
        "model.add(embedding_layer)\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(21, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rHU3ZhNjPOe"
      },
      "source": [
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yTsfeTOqeTV"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2290U-Y6gl9"
      },
      "source": [
        "model.save('/content/drive/MyDrive/models/NLP_GloVeEmbedding.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V0m5rXpq-TM"
      },
      "source": [
        "#using a CNN custom\n",
        "from keras.layers import Conv1D \n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(vocab_size, 100, input_length=maxlen))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(21, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1P_svQQ6nYB"
      },
      "source": [
        "model.save('/content/drive/MyDrive/models/NLP_custom_CNN.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu4eSB6MrslZ"
      },
      "source": [
        "#using a CNN with GloVe embedding\n",
        "from keras.layers import Conv1D \n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "model.add(embedding_layer)\n",
        "\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(21, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka5KBttD796B"
      },
      "source": [
        "model.save('/content/drive/MyDrive/models/NLP_GloVe_CNN.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvlxBhmgszDe"
      },
      "source": [
        "#Using LSTM model with custom embedding\n",
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=maxlen))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(21, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3rLYPf6seAC"
      },
      "source": [
        "#Using LSTM model with GloVe embedding\n",
        "\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(21, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSUEUvakdow-"
      },
      "source": [
        "Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGAFSxs099V9"
      },
      "source": [
        "train_embedding_weights = X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8zcqw0lZzP9"
      },
      "source": [
        "from keras.layers import concatenate\n",
        "from keras.layers import Input\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=False, extra_conv=False):\n",
        "    \n",
        "    embedding_layer = Embedding(vocab_size, 100, input_length=maxlen)\n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
        "    convs = []\n",
        "    filter_sizes = [3,4,5]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "    l_merge = concatenate([convs[0],convs[1],convs[2]],axis=1)\n",
        "\n",
        "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
        "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
        "    pool = MaxPooling1D(pool_size=3)(conv)\n",
        "\n",
        "    if extra_conv==True:\n",
        "        x = Dropout(0.5)(l_merge)  \n",
        "    else:\n",
        "        # Original Yoon Kim model\n",
        "        x = Dropout(0.5)(pool)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    # Finally, we feed the output into a Sigmoid layer.\n",
        "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
        "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
        "    preds = Dense(21,activation='softmax')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adadelta',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuXDnDqSbHyu"
      },
      "source": [
        "model = ConvNet(train_embedding_weights, maxlen, 35987, 250, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO_7rQ8rcZYz"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64,validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeWrntXU-jeM"
      },
      "source": [
        "#Trying another model - also 5%, very slow to train\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,LSTM,GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=maxlen))\n",
        "model.add(GRU(units = 32, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(21, activation = 'softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJMI0ybPEqr6"
      },
      "source": [
        "#another CNN model, custom\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=maxlen))\n",
        "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(21, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY8WyfJFAOdZ"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=6, batch_size=128,validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76PQuSUmFW7k"
      },
      "source": [
        "#another CNN model, GloVe\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=True))\n",
        "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(21, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=256,validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}